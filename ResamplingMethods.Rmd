---
title: "Resampling Methods"
author: "Greg Cooke"
date: "10/08/2020"
output: html_document
---

# Introduction {-}

In the grand scheme of the data pipeline, one would look to perform resampling after creating models from the data. The objective of such activity is to obtain additional information about the model and estimate variability (or quantify confidence - however you want to put it).

The two main processes in resampling are *cross-validation* and *bootstrapping*, a quick explanation:

- *Cross-validation* is estimating the test error in order to measure the performance of the model (known as *model assesment*). You can use it to find the minimum point in estimated MSE.
- *Bootstrapping* is estimating the accuracy of the parameters or the statistical learning method (SLM) itself.

# Cross Validation {-}

With any model, you need to train it and then test it (sorry for keeping it simple). To do so you need to split your available dataset into 2 segments, training and testing (again, sorry for being obvious) where the model is trained on the former set and the latter set is held out to find a *test error rate*. Doing this multiple times allows you to fit the data as accurately as possible. There are three methods I'm going to look at: *Validation Set Approach*, *Leave-One-Out Cross Validation* and *k-Fold Cross Validation*.

I'm going to use the famous *Auto* dataset (I'm ripping the idea straight from the ISLR package and book - thanks Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani) with *recipes* package to create models and the *rsample* package to cross validate them - both can be found in the within the *tidymodels* ensemble.

```{r, message = FALSE}

library(ISLR)
library(dplyr)
library(tidyr)
library(ggplot2)
library(caret)
library(rsample)
library(purrr)
library(plyr)
library(boot)
require(gridExtra)

set.seed(123)

```

As written in the book, there is some proof (shown below which I got from [here](https://www.andrew.cmu.edu/user/achoulde/95791/labs/Lab02_solutions.html#the-validtion-set-approach) that $horsepower^2$ is a better estimate of *mpg* than *horsepower* which creates the obvious question of whether $horsepower^3$, $horsepower^4$ etc leads to further improvements in explanatory power.

 <center>

```{r, echo = F, message = F, warning = F}

ggplot(data = Auto, aes(x = horsepower, y = mpg)) +
  geom_point() +
  stat_smooth(method = "lm", aes(colour = "Linear Regression")) + 
  stat_smooth(method = "lm", formula = y ~ poly(x, 2), aes(colour = "Quadratic Fit")) +
  stat_smooth(method = "lm", formula = y ~ poly(x, 4), aes(colour = "Quartic Fit")) +
  labs(x = "Horsepower",
       y = "MPG",
       title = "Horsepower on MPG using linear, quadratic and quartic lines")
```

</center>

## Validation Set Approach

This is the simplest out of the three approaches I'll be looking at. The idea here is to just create multiple random splits in the data with the standard aim of minimizing the MSE. For *n* observations $\frac{n}{2}$ will be in the training set and so (again, I promise it'll become more complicated later) $\frac{n}{2}$ will be in the testing set.

I have created a function to create the tables which will store the results of each method below.

```{r}

# Create Table Function

tableCreatR <- function(c_list){
  
mse <- data.frame(matrix(ncol = length(c_list), 
                         nrow = 0))
    
colnames(mse) <- c_list
  
return(mse)

}

```

I'm sure my following code would have broken some unwritten R rules on nesting loops and I'm very happy to hear alternatives to this method but this following code shows how MSE is effected by powers on the *Horsepower* variable and I have used 10 separate (and random) splits in the data to create the graph below.

```{r}

mse_vsa <- tableCreatR(c_list = c("MSE_Estimate",
                                  "Horsepower_Power",
                                  "Fold",
                                  "Time_Taken"))

data_vsa <- list()

for(y in 1:10){

train <- sample(nrow(Auto), 0.5*nrow(Auto)) 

for(x in 1:8){
  
t <- system.time({
  
model_vsa <- lm(mpg ~ poly(horsepower, x), 
                data = Auto, 
                subset = train)
  
})    

mse_vsa[nrow(mse_vsa)+1,] <- c(with(Auto, mean((mpg - predict(model_vsa, Auto))[-train]^2)), #MSE
                               x, # Power
                               y, # Fold
                               t[3]) # Time

}

}

```

What's a result without a visualization? Not much.

<center>

```{r,echo = FALSE}

  ggplot(mse_vsa) + 
  geom_line(aes(x = Horsepower_Power, 
                y = MSE_Estimate, 
                colour = as.factor(Fold))) +
  labs(y = "Mean Squared Error",
       title = "Horsepower_Power on the MSE with 10-fold VSA") +
  scale_colour_discrete("Fold") +
  scale_x_continuous("Power", 
                     labels = as.character(mse_vsa$Horsepower_Power), 
                     breaks = mse_vsa$Horsepower_Power)

```

</center>

What this plot shows is the variability in MSE when using a Validation Set Approach. This is due to the large changes in the observations that exist in each set when a new sample is taken. The other issue is that statistical models perform worse when fewer observations are used (in this case, 50% of the possible sample) which means the MSE is very likely to be exaggerated (or *overestimated*).

## Leave-One-Out Cross Validation

Considering VSA's criticisms, LOOCV overcomes them in its methodology. Like VSA, the data (size *n*) is split into
training and testing but, in this case, the training set is size *n-1* which aims to predict the singular dependent variable that has been excluded from the testing set. This is repeated *n* times until all datum have been used as the test variable. Unlike VSA, once the row has been used in the test set it will not be used again.

The $MSE_{i}$ is unbiased as it is being trained on *n-1* observations (almost the entire dataset), which means by following the bias-variance tradeoff, it has high variance. This can be seen because it is based off a single, test observation. The LOOCV estimate for MSE is:

<center>

$CV_{(n)} = \frac{1}{n}\sum^{n}_{i=1}MSE_{i}$

</center>

For this we will be using the *caret* package in the approach as *rsample*'s version doesn't work as well from some research. (I have borrowed code from [here](https://stackoverflow.com/questions/41742777/using-poly-function-within-training-model-in-caret-package-resulting-in-datafra))

```{r}

mse_loocv <- tableCreatR(c_list = c("MSE_Estimate",
                                    "Horsepower_Power",
                                    "Time_Taken"))

for (x in 1:8){

t <- system.time({  
  
model_loocv <- train(as.formula(bquote(mpg ~ poly(horsepower, .(x)))), 
                     method = "lm", 
                     data = Auto, 
                     trControl = trainControl(method = "LOOCV"))

})

mse_loocv[nrow(mse_loocv)+1,] <- c(model_loocv$results$RMSE^2,
                                   x,
                                   t[3])

}

```

Let's visualise this:

<center>

```{r,echo = FALSE}

  ggplot(mse_loocv) + 
  geom_line(aes(x = Horsepower_Power, 
                y = MSE_Estimate)) +
  labs(y = "Mean Squared Error",
       title = "Horsepower_Power on the MSE with LOOCV") +
  scale_x_continuous("Power", 
                     labels = as.character(mse_loocv$Horsepower_Power), 
                     breaks = mse_loocv$Horsepower_Power)

```

</center>

So it solves the VSA problems as training with *n-1* observations almost eradicates the problem of overestimation of errors and there is no variance from training/testing sets as it's done *n* times! Another fantastic advantage is that it can be used with any predictive model from logistic regression to LDA. Great news right? Well there is a catch...

*Auto* is quite a small dataset with only 392 rows and the model I'm running is a simple regression so it runs in very little time at all. There is serious potential that *LOOCV* becomes computationally very expensive as n moves towards the extremely large or the model becomes more complex.

## k-Fold Cross Validation

The easiest way to put it is that the obsverations are split into *k* sets (*LOOCV* is sort of a version of *k-fold CV* where *k=n*). *k-1* of these sets are used for training and the process is repeated *k* times. In the *rsample* package the function is called *vfold_cv*, just trying to keep you on your toes with that one I assume.

The obvious advantage here is computational (comparative to *LOOCV* at least). 

```{r}

mse_kfcv <- tableCreatR(c_list =  c("MSE_Estimate",
                                   "Horsepower_Power",
                                   "k_folds",
                                   "Time_Taken"))

for (k in seq(from = 5, 
              to = 20, 
              by = 5)){

data_kfcv <-  rsample::vfold_cv(Auto,
                                k)

 for (x in 1:8){

t <- system.time({   
   
  model_kfcv <- map(data_kfcv$splits, ~ lm(mpg ~ poly(horsepower, x),
                                           data = .))

})  
  
  mse_kfcv[nrow(mse_kfcv)+1,] <- c(mean(model_kfcv[[1]]$residuals^2, na.rm = T),
                                 x,
                                 k,
                                 t[3])

               }

}

```

<center>

```{r,echo = FALSE}

  ggplot(mse_kfcv) + 
  geom_line(aes(x = Horsepower_Power, 
                y = MSE_Estimate,
                colour = as.factor(k_folds))) +
  labs(x = "Power",
       y = "Mean Squared Error",
       title = "Horsepower_Power on the MSE with k-Fold Cross Validation") +
  scale_colour_discrete("k-Folds") +
  scale_x_continuous("Power", 
                     labels = as.character(mse_kfcv$Horsepower_Power), 
                     breaks = mse_kfcv$Horsepower_Power)


```

</center>

*k-Fold* is a better estimate of test error rate than *LOOCV* which seems bizarre from a simple glance. Consider again the *bias-variance* tradeoff, there is a high bias in the *VSA* approach because it only uses $\frac{n}{2}$ observations to train the model and there is little/no bias in *LOOCV* because it uses *n-1* observations with *k-Fold* sitting between the 2 with regards to bias reduction.

Remembering that *variance* is the amount that $\hat{f}$ would change if we used a different training data set, *LOOCV* has a higher variance as it averages the output of *n*, almost identical and positively correlated models. The mean of many highly correlated models quantities has a high variance. As $k < n$, k models have less of a correlation with each other and so do not suffer as badly from this variance issue. Being in the middle of *VSA* and *LOOCV* they neither suffer from overly high bias or overly high variance.

## Computational Differences

So I've mentioned a few times about the computational differences between methods and while I show my results through using the *system.time()* base function I want to stress that I have no doubt there are more efficient ways to code the various methods I have shown above and this is only from my findings.

<center>

```{r, echo = F, message = F, warning = F}

# Tidy to focus on time

mse_kfcv_time <- mse_kfcv %>%
  dplyr:: filter(k_folds %in% c(5,20)) %>%
  tidyr:: spread(k_folds, Time_Taken) %>%
  dplyr:: rename(Five_kFold = `5`,
                 Twenty_kFold = `20`) %>%
  dplyr:: group_by(Horsepower_Power, 
                    .groups = 'drop') %>%
  dplyr:: summarise(Five_kFold = sum(Five_kFold, na.rm = T),
                    Twenty_kFold = sum(Twenty_kFold, na.rm = T))
  
mse_vsa_time <- mse_vsa %>%
  dplyr:: group_by(Horsepower_Power) %>%
  dplyr:: summarise(`VSA (x10)` = sum(Time_Taken, na.rm = T), 
                    .groups = 'drop')

mse_loocv_time <- mse_loocv %>%
  dplyr:: select(-MSE_Estimate) %>%
  dplyr:: rename(LOOCV = Time_Taken)

# Join all together

mse_total <- base:: Reduce(function(x,y) merge(x,y,
                                        by="Horsepower_Power",
                                        all=TRUE),
                        list(mse_vsa_time,
                            mse_loocv_time,
                            mse_kfcv_time)) %>%
  tidyr:: gather(Method, Time, -Horsepower_Power)

# Viz

ggplot2:: ggplot(mse_total, aes(x = Horsepower_Power,
                      y = Time,
                      fill = Method)) +
  geom_bar(stat='identity',
           position=position_dodge()) +
  scale_x_continuous("Horsepower_Power", 
                     labels = as.character(mse_total$Horsepower_Power), 
                     breaks = mse_total$Horsepower_Power) +
  labs(x = "Power",
       y = "Time (s)",
       title = "Difference in computing time for different CV method")

```

</center>

As expected, the *LOOCV* technique is by far the most computationally expensive - perhaps exageratted by using a different package but still the results are significant. We did show that it did produce the best MSE though, it's fair to say that payoff and compromise are littered throughout Data Science functionality.

# Bootstrapping {-}

So moving on to *bootstrapping*, as aforementioned the objective of this technique is to find the level of accuracy associated with a parameter or SLM itself. It creates mulitple samples out of your dataset and replaces each time meaning there may be duplications (this isn't an issue due to the large amount of samples you crate). Bootstrapping doesn't assume any shape of your data which means you can use it for a wider range of distributions and smaller sample sizes. It's very transferrable and can be used on a range of SLMs, obviously we see standard errors from some models automatically but in the cases of more flexible (and thus variable models) this can be harder to obtain without bootstrapping.

Again, like ISLR, we are going to use the *Portfolio* dataset from the *ISLR* package.

The problem is *Minimum Variance Two Asset Portfolio* of assets *A* and *B* and just because I get a kick out of it I'll do some solving to produce the mathematical problem below:

<center>

$w_B =  1 - w_A$ (1)

The return is: $E[R_P] = w_AE[R_A] + w_BE[R_B]$ (2) 

The variance is: $Var[R_P] = w_A^2 \sigma_{A}^2+w_B^2\sigma_{B}^2+2w_1w_2\sigma_{AB}$ (3)

Substituting (1) into (3): $Var[R_P] = \sigma_{B}^2 + 2w_A(\sigma_{AB} - \sigma_{B}^2) + w_A^2(\sigma_{A}^2 +\sigma^2_B-2\sigma_{AB})$ (4)

The derivative of (4) with respect to $w_A$ is: $\frac{dVar[R_P]}{dw_A} = 2w_{A}(\sigma_A^2 + \sigma_B^2 - 2\sigma_{AB}) + 2(\sigma_{A,B} - \sigma_{A}^2)$ (5)

Set (5) equal to zero: $w_A = \frac{\sigma_{B}^2 - \sigma_{AB}}{\sigma_{A}^2 +\sigma^2_B-2\sigma_{AB}}$

</center>

The aim of the following is now to find an estimate for $w_A$ (and thus $w_B$) from our dataset. I have created a function below which finds our variable of interest.

```{r}

weightFindR <- function(df, ind){
  
  var_x <- var(df$X[ind])
  var_y <- var(df$Y[ind])
  cov_xy <- cov(df$X[ind], df$Y[ind])
  
  wA <- (var_y - cov_xy)/(var_x + var_y - 2*(cov_xy))
    
  return(wA)
  
}

```

Bootstrapping is repeatedly obtaining distinct datasets by continously sampling from the original dataset (being *Portfolio* in this case). The sampling is performed with replacement which means you can have the same observation more than once in a bootstrapped dataset.

I'm using the *boot* function from its eponymously named package for this example and then I will use *rsample*'s *bootstraps* function after for a different example.

```{r}

boots <- boot(Portfolio, weightFindR, R=1000)

boots 

```

What this shows us is that using the original dataset, 1000 bootstrapped sample sets have created an estimated value for $w_A$ ($\hat{w_A}$) at 0.5758 with a standard error error $SE(\hat{w_A})$ at 0.0937. Below there are plots of the estimated coeffients and how they are spread out. I decided to plot it using ggplot for aethestic reasons (and also it's a bit of a fun challenge converting base plots to ggplots), it gave us:

<center>

```{r, echo = F}

t <- as.data.frame(boots$t)

h <- ggplot(t, aes(x = V1)) +
  geom_histogram(colour = "black", binwidth = 0.02) + 
  geom_vline(data = data.frame(measurement = "True wA",
                               value = weightFindR(Portfolio, 1:100)), 
             aes(xintercept = value, color = measurement), size = 1) +
  labs(x = "t*",
       y = "Count",
       title = "Histogram")

q <- ggplot(t, aes(sample = V1)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Quantiles of Standard Normal",
       y = "t*",
       title = "Q-Q Plot")

grid.arrange(h, q, ncol=2, top="Bootstrap Graphs for Portfolio")

```

The Histogram shows us the distribution of the parameter estimates that we've produced through the bootstrap. The QQ plot shows us how normal that distribution is (a really good article on QQplots is [here](https://data.library.virginia.edu/understanding-q-q-plots/)).

</center>

So now that we've shown we can find fairly accurate estimates for variables using this technique I'm now going to try and do the same with a simple linear model. Let's go back to the model we were looking act regarding cross-validation, $mpg \sim horsepower + horsepower^2$ (we're using the variation that gave the lowest MSE according to *LOOCV*). Unsurprisingly, this [page](https://www.tidymodels.org/learn/statistics/bootstrap/) really helped me with understanding writing the code.

```{r}

boots2 <- bootstraps(Auto, times = 1000, apparent = TRUE)

modelFitR <- function(split) {
  
lm(mpg ~ horsepower, data = analysis(split))

}

coef_boot <- boots2 %>%
  dplyr:: mutate(model = map(splits, modelFitR),
                 coef = map(model, tidy)) %>%
  tidyr:: unnest(coef)

head(coef_boot,10)

```

From this we can graph confidence intervals and begin to really see the variability of the proposed parameter estimates.

```{r}



```


# Readings {-}

I have already added links to a few pages where I have lifted code and edited it to fit my objectives, more indirectly those these pages have helped with my technical understanding and coding technique.

https://alison.rbind.io/post/2020-02-27-better-tidymodels/
https://daviddalpiaz.github.io/r4sl/
https://juliasilge.com/blog/intro-tidymodels/
https://blog.methodsconsultants.com/posts/understanding-bootstrap-confidence-interval-output-from-the-r-boot-package/


